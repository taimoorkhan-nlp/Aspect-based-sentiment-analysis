{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ab8f3db-691a-4f75-bc29-91092e48289f",
   "metadata": {},
   "source": [
    "# Aspect-based sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e578a5-b3de-473e-90cb-32b3bb3db30c",
   "metadata": {},
   "source": [
    "Aspect-Based Sentiment Analysis (ABSA), also known as fine-grained opinion mining, focuses on identifying the sentiment of a text concerning a specific aspect. This approach has gained prominence as a response to the limitations of traditional sentiment analysis methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f46707-b6ef-45ea-ade2-ebcc8edffb13",
   "metadata": {},
   "source": [
    "Conventional sentiment analysis typically assigns an overall sentiment label (e.g., positive, negative, or neutral) to an entire text. While this is sufficient for many applications, it lacks the granularity needed in scenarios where sentiment varies across different aspects. For instance, in a restaurant review, a customer may rate the restaurant positively overall but criticize the service. In such cases, ABSA helps capture the sentiment towards individual aspects, such as identifying that the sentiment toward “service” is negative, despite the overall positive review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cc0115-4d20-4429-8fe3-60bf3a42ef5f",
   "metadata": {},
   "source": [
    "## Example: Hotel reviews\n",
    "\n",
    "<p style=\"margin-left:200px; float:right\"><img src=\"hotel-reviews.jpg\" width=\"300px\" /></p>  \n",
    "\n",
    "**Hotel aspects:** cleanliness, staff behavior, food quality, location, service and amenities\\\n",
    "**Aspect sentiment analysis:** sentiment scores aggregated for each aspect\n",
    "\n",
    "\n",
    "\n",
    "- cleanliness_tables_area: ⭐⭐⭐⭐⭐\n",
    "- staff_behavior_serving_greeting: ⭐⭐⭐\n",
    "- food_menu_taste_cousines: ⭐⭐⭐⭐⭐\n",
    "- location_by main road_close to city: ⭐⭐\n",
    "- service_waiter_table booking: ⭐⭐\n",
    "\n",
    "\n",
    "Aspect-Based Sentiment Analysis (ABSA) is particularly useful in analyzing hotel reviews, where customers express opinions on multiple aspects of their stay, such as cleanliness, staff behavior, room quality, location, and amenities. Traditional sentiment analysis may label a review as positive or negative as a whole, but ABSA allows for a more nuanced understanding by identifying sentiment tied to specific aspects. For example, a guest might praise the hotel's location and service but complain about the room's cleanliness. By applying ABSA, hotel management can gain detailed insights into what aspects need improvement while maintaining strengths. Additionally, potential customers can make informed decisions based on sentiments about aspects that matter most to them. This fine-grained analysis helps hotels enhance customer experience and tailor their services to meet guest expectations more effectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22016adf-5bcf-41d3-bb4a-5b85a514baba",
   "metadata": {},
   "source": [
    "Aspect-Based Sentiment Analysis (ABSA) is a challenging task as it involves both identifying relevant \"aspects\" within a text and assigning sentiment labels to them. Various approaches exist for ABSA, but a common strategy involves first detecting aspects in the text and then applying an ABSA model to determine the sentiment associated with each aspect.\n",
    "\n",
    "Aspect identification can be performed using different techniques, including rule-based methods such as dictionary-based approaches. For instance, terms like \"iPhone X\" or \"MacBook Pro\" might be predefined as aspects.\n",
    "\n",
    "After identifying aspects, an ABSA classifier is trained to assess sentiment in relation to the context of a sentence. For example, in the sentence, \"We had a great experience at the restaurant, the food was delicious, but the service was kinda bad,\" the classifier would determine that the sentiment towards \"service\" is negative, despite the overall positive tone of the review.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd15fbf5-3147-4797-b042-2c015911c0bf",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "Topic modeling is an unsupervised machine learning technique used to identify hidden thematic structures in a large collection of text data. It helps discover topics that frequently occur in a dataset without requiring prior labeling or annotation. One of the most widely used topic modeling methods is Latent Dirichlet Allocation (LDA), which represents documents as mixtures of topics, with each topic consisting of a set of words with varying probabilities. Topic modeling is commonly applied in text mining, information retrieval, document classification, and content recommendation systems. It enables researchers and businesses to analyze vast amounts of textual data, uncover trends, and gain insights into discussions, making it a valuable tool in areas such as social media analysis, academic research, and customer feedback categorization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1b42fd-6323-4953-8b68-e6fb698a560e",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "Sentiment analysis, also known as opinion mining, is a natural language processing (NLP) technique used to determine the sentiment or emotional tone expressed in a piece of text. It involves classifying text into categories such as positive, negative, or neutral, enabling businesses and researchers to analyze opinions, feedback, and trends. Sentiment analysis is widely applied in various domains, including social media monitoring, customer feedback analysis, brand reputation management, and market research. Advanced sentiment analysis techniques, such as deep learning and transformer-based models, enhance accuracy by capturing contextual nuances, sarcasm, and complex emotions within text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f155faf-9f0f-4701-9ffe-c13371f87a2e",
   "metadata": {},
   "source": [
    "## Tutorial Content\n",
    "1. Data preparation / preprocessing\n",
    "2. Integer encoding\n",
    "3. Topic modeling (Latent dirichlet allocation with collapsed Gibbs sampling)\n",
    "4. Performing sentiment analysis (using [SentiStrength](https://github.com/zhunhung/Python-SentiStrength))  \n",
    "5. Separate neutral i.e., topic (aspect) presenting words and subjective words\n",
    "6. Aggregating scores of the subjective words against each topic\n",
    "7. Preparing output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dde5b04-a301-4428-bc63-8337682e9f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Its vanilla implementation of Topic modeling that only uses basic tools:\n",
    "# json - to read from and write to files in json format \n",
    "# numpy - for faster matrix operations \n",
    "# pandas - to read csv data\n",
    "# string - to only keep English letters, removing puntuations and other characters\n",
    "# random - to generate random numbers for initializing Markov-chain monte carlo, and \n",
    "#           and during algorithm working to avoid local optima\n",
    "\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb1a850-6841-48b5-a7a0-0e60100005dd",
   "metadata": {},
   "source": [
    "### 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd84f4a-cd45-4763-81d6-14f5d8ac7dcd",
   "metadata": {},
   "source": [
    "**1.1. Read textual data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcd750c1-ee37-44b1-af9a-b22754c49757",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "def clean_text(text):\n",
    "    clean_text = text.lower()\n",
    "    # cleaning documents by removing unwanted characters\n",
    "    clean_text = \"\".join([char for char in text if char in string.ascii_lowercase])\n",
    "    # cleaning documents by stopwords\n",
    "    clean_text = [word for word in text.split(\" \") if word not in en_stopwords and len(word) > 2]\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5689664b-6ad3-4677-b7e5-b52b0b8edf08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "605"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read input data: titles of BBC articles available on the following link\n",
    "# https://github.com/vahadruya/Capstone-Project-Unsupervised-ML-Topic-Modelling/blob/main/Input_Data/input.csv\n",
    "\n",
    "with open('config.json', 'r') as file:\n",
    "    configurations = json.load(file)\n",
    "#df = pd.read_csv(configurations[\"text-doc-path\"], encoding=\"utf-8\")\n",
    "rawdata = open(configurations[\"text-doc-path\"]).read()\n",
    "#rawdata = df[\"Title\"].tolist()\n",
    "rawdata = rawdata.split(\"\\n\")\n",
    "\n",
    "# Tokenize sentences into words\n",
    "tokenized_documents = []\n",
    "for document in rawdata: # Considering only first 100 titles for the sake of demonstration\n",
    "    document = clean_text(document)\n",
    "    if len(document) > 2:\n",
    "        tokenized_documents.append(document)\n",
    "len(tokenized_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fe0342-b2d5-482d-97be-3e874e2975ea",
   "metadata": {},
   "source": [
    "**1.2. Generate Integer encoding**\\\n",
    "It preserves both frequency and position related information. The process involves assigning each unique token a dedicated integer id, preserving it in a dictionary for later retrieval, while rewriting documents by replacing with with their integer ids.\n",
    "\n",
    "It makes the operations a lot faster as numbers are much faster to read/store and compare as compared to strings. \n",
    "\n",
    "The integer ids will be replaced with their original words at the end using stored dictionary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3a5beda-d1f7-45f6-82fb-c3db145b1ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of unique tokens and assign integers\n",
    "dictionary = {}\n",
    "revdictionary = {}\n",
    "index = 0\n",
    "\n",
    "#tokenized_documents = [[word for word in doc if word not in esw] for doc in tokenized_documents]\n",
    "\n",
    "for doc in tokenized_documents:\n",
    "    for word in doc:\n",
    "        if word not in dictionary.keys():\n",
    "            dictionary[word] = index\n",
    "            revdictionary[index] = word\n",
    "            index += 1\n",
    "\n",
    "# Replace words in sentences with their corresponding integers\n",
    "encoded_documents = [[dictionary[word] for word in doc] for doc in tokenized_documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25a6af2-af1e-4011-bf46-6ea25b151ef3",
   "metadata": {},
   "source": [
    "**1.3. Storing intermediate data**\\\n",
    "The integer encoded documents are stored in files\n",
    "the word-to-id and id-to-word dictionaries are also stored\n",
    "\n",
    "*It will help to avoid these steps, each time topic modeling is performed under different settings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33910370-3787-40f8-ad5f-6c72028d6513",
   "metadata": {},
   "outputs": [],
   "source": [
    "toStr = ''\n",
    "for endoc in encoded_documents:\n",
    "    toStr = toStr + '\\t'.join(str(item) for item in endoc)\n",
    "    toStr = toStr + '\\n'\n",
    "toStr = toStr[:-2]\n",
    "file = open('data/integer-encoded-data.txt', 'w')\n",
    "file.write(toStr)\n",
    "file.close()\n",
    "\n",
    "#write dictionary to file\n",
    "file = open('data/dictionary.json', 'w')\n",
    "file.write(json.dumps(dictionary))\n",
    "file.close()\n",
    "file = open('data/revdictionary.json', 'w')\n",
    "file.write(json.dumps(revdictionary))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e58208-8423-4bfd-9385-164a2787774c",
   "metadata": {},
   "source": [
    "### 2. Integer Encoding\n",
    "\n",
    "It preserves both frequency and position related information. The process involves assigning each unique token a dedicated integer id, preserving it in a dictionary for later retrieval, while rewriting documents by replacing with with their integer ids.\n",
    "\n",
    "It makes the operations a lot faster as numbers are much faster to read/store and compare as compared to strings. \n",
    "\n",
    "The integer ids will be replaced with their original words at the end using stored dictionary files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7118d759-68ed-4f06-b816-cb6c4ee6bb59",
   "metadata": {},
   "source": [
    "**2.1 Generate integer encoded documents**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73d48060-c97c-4d45-a27d-e8f225078aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of unique tokens and assign integers\n",
    "dictionary = {}\n",
    "revdictionary = {}\n",
    "index = 0\n",
    "\n",
    "#tokenized_documents = [[word for word in doc if word not in esw] for doc in tokenized_documents]\n",
    "\n",
    "for doc in tokenized_documents:\n",
    "    for word in doc:\n",
    "        if word not in dictionary.keys():\n",
    "            dictionary[word] = index\n",
    "            revdictionary[index] = word\n",
    "            index += 1\n",
    "\n",
    "# Replace words in sentences with their corresponding integers\n",
    "encoded_documents = [[dictionary[word] for word in doc] for doc in tokenized_documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3107372c-457a-42ce-b270-fa6163672f68",
   "metadata": {},
   "source": [
    "**2.2 Storing intermediate data**\\\n",
    "The integer encoded documents are stored in files\n",
    "the word-to-id and id-to-word dictionaries are also stored\n",
    "\n",
    "*It will help to avoid these steps, each time topic modeling is performed under different settings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b63472d6-eda8-4780-b3f6-cb75a439d2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "toStr = ''\n",
    "for endoc in encoded_documents:\n",
    "    toStr = toStr + '\\t'.join(str(item) for item in endoc)\n",
    "    toStr = toStr + '\\n'\n",
    "toStr = toStr[:-2]\n",
    "file = open('data/integer-encoded-data.txt', 'w')\n",
    "file.write(toStr)\n",
    "file.close()\n",
    "\n",
    "#write dictionary to file\n",
    "file = open('data/dictionary.json', 'w')\n",
    "file.write(json.dumps(dictionary))\n",
    "file.close()\n",
    "file = open('data/revdictionary.json', 'w')\n",
    "file.write(json.dumps(revdictionary))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6379e7ed-c3e2-4e9f-9516-2ac164eba45d",
   "metadata": {},
   "source": [
    "### 3. Topic Modeling (Latent Dirichlet Allocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105028e2-a7dd-4f00-8031-ccd9e25e2887",
   "metadata": {},
   "source": [
    "**Setting (in config.json)**\n",
    "\n",
    "*numTopics: 10* - how much can we stretch the data? After manual exploration or domain knowledge having fewer topics more than the high level separation can give good meaningful topics. Having more topics beyond that can identify more specific topics, however there can me more topics that are incoherent and cannot be interpreted.\n",
    "\n",
    "*numAlpha ($\\alpha$): 1.0* - We want natural representation of topics in documents. A higher value will push in more topics within documents while a lower value will only have fewer most dominant topics. $\\alpha$ is a hyper-parameter where a higher value (above 1) adds external bias to each topic within a document. In extreme case (a value of 1000 or above for example) will have equal representation of all topics within the document.\n",
    "\n",
    "*numBeta ($\\beta$): 0.01* - We want fewer words to represent a topic, therefore, a value 0.01 (below 1) is used. Given the vocabulary size, a lower value will push the lower probability words in the topic further down, therefore, we will have few more prominent words to represent a topic. Pushing this value further down will results in increase in the probability of the prominent words while further drop in the probabilities of the background words for the topic.\n",
    "\n",
    "Further, we set the number of iterations *numGIterations: 1000* giving it enough time to settle, starting from a randomly initialized state.\n",
    "\n",
    "There are some other performance related parameters, set to default values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74085e89-4212-4d95-8ca0-ce810b7ebe47",
   "metadata": {},
   "source": [
    "**LDA class**\n",
    "main functions are:\n",
    "1. Markov chain monte carlo initialization (giving the model a random inital state, expecting the model\n",
    "    to converge for higher number of iterations.\n",
    "2. Collapsed gibbs sampling inference: in each iteration \\\n",
    "   2.1 Iterates through all documents, all tokens/words in each document \\\n",
    "   2.2 For for each token computes its most suitable topic, given the current status of the model \\\n",
    "   2.3 Updates new topic if different from current topic, associated estimates update, so does the model state \\\n",
    "3. Estimate document-topic distribution from the final state of the model \n",
    "4. Estimate topic-word distribution (organized in decreasing order of probabilities) from the final state of the model\n",
    "5. Other utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b45065a-81e2-49f8-b3de-cb1db117ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The class implements topic modeling (Latent dirichlet allocation) algorithm using collapsed gibbs sampling as in inference. \n",
    "class LDA:\n",
    "    # topics to extract from the data (Components)\n",
    "    _numTopics = None\n",
    "    # vocabulary (unique words) in the dataset\n",
    "    _arrVocab = None\n",
    "    #size of vocabulary (count of unique words)\n",
    "    _numVocabSize = None\n",
    "    # dataset\n",
    "    _arrDocs = []\n",
    "    # dataset size (number of documents)\n",
    "    _numDocSize = None\n",
    "    # dirichlet prior (document to topic prior)\n",
    "    _numAlpha = None\n",
    "    # dirichlet prior (topic to word prior)\n",
    "    _numBeta = None\n",
    "    _ifScalarHyperParameters = True\n",
    "    # Gibb sampler iterations\n",
    "    _numGSIterations = None\n",
    "    # The iterations for initial burnin (update of parameters)\n",
    "    _numNBurnin = None\n",
    "    # The iterations for continuous burnin (update of parameters)\n",
    "    _numSampleLag = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    # The following attributes are for internal working\n",
    "    __numTAlpha = None  \n",
    "    __numVBeta = None   \n",
    "    __arrTheta = None\n",
    "    __arrThetaSum = None\n",
    "    __arrPhi = None\n",
    "    __arrPhiSum = None\n",
    "    __arrNDT = None\n",
    "    __arrNDSum = []\n",
    "    __arrNTW = None\n",
    "    __arrNTSum = []\n",
    "    __arrZ = []\n",
    "    \n",
    "    # for alpha to be a list, its size must be equal to the size of the dataset, has value for each doc\n",
    "    # for beta to be a list, its size must be equal to the number of topics, has value for each topic  \n",
    "    def __init__(self, numTopics = 2, numAlpha = 1.0, numBeta = 0.01, \n",
    "                 numGSIterations = 1000, numNBurnin = 50, numSampleLag = 20, \n",
    "                 wordsPerTopic = 10):\n",
    "        self._numTopics = configurations[\"numTopics\"]\n",
    "        self._numAlpha = configurations[\"numAlpha\"]\n",
    "        self._numBeta = configurations[\"numBeta\"]\n",
    "        self._numGSIterations = configurations[\"numGSIterations\"]\n",
    "        self._numNBurni = configurations[\"numNBurnin\"]\n",
    "        self._numSampleLag = configurations[\"numSampleLag\"]\n",
    "        self.__wordsPerTopic = configurations[\"wordsPerTopic\"]\n",
    "            \n",
    "    #load data as integer encoding of words in a sequence (no padding or truncation)\n",
    "    def getData(self, path):\n",
    "        file = open(path, 'r')\n",
    "        rawData = file.read()\n",
    "        file.close()\n",
    "        self.__loadData(rawData)\n",
    "        self.__loadVocab()\n",
    "        self.__prepareCollections()\n",
    "\n",
    "    #load docs and docSize from the dataset\n",
    "    def __loadData(self, rawData):\n",
    "        rows = rawData.split('\\n')\n",
    "         \n",
    "        #read dataset as documents of words IDs\n",
    "        for row in rows:\n",
    "            swordlist = row.split('\\t')\n",
    "            swordlist = list(filter(None, swordlist))   #remove empty items from list\n",
    "            if len(swordlist) > 0:\n",
    "                iwordlist = [eval(w) for w in swordlist]    \n",
    "                self._arrDocs.append(iwordlist)\n",
    "\n",
    "        # determine dataset size\n",
    "        self._numDocSize = len(self._arrDocs)\n",
    "        \n",
    "        \n",
    "    #Determine unique words (vocabulary) and count of unique words (vocabSize)    \n",
    "    def __loadVocab(self):\n",
    "        #determine unique vocabulary\n",
    "        uniqueWords = []\n",
    "        for doc in self._arrDocs:\n",
    "            for word in doc:\n",
    "                if word not in uniqueWords:\n",
    "                    uniqueWords.append(word)\n",
    "        self._arrVocab = uniqueWords\n",
    "        self._numVocabSize = len(self._arrVocab)    \n",
    "\n",
    "    def __prepareCollections(self):\n",
    "        self.__arrNDSum = np.array([0] * self._numDocSize)\n",
    "        self.__arrTheta = np.array([[0] * self._numTopics] * self._numDocSize)\n",
    "        self.__arrThetasum = np.array([[0] * self._numTopics] * self._numDocSize)\n",
    "        self.__arrNDT = np.array([[0] * self._numTopics] * self._numDocSize)\n",
    "        \n",
    "        self.__arrNTSum = np.array([0] * self._numTopics)\n",
    "        self.__arrPhi = np.array([[0] * self._numVocabSize] * self._numTopics)\n",
    "        self.__arrPhisum = np.array([[0] * self._numVocabSize] * self._numTopics)\n",
    "        self.__arrNTW = np.array([[0] * self._numVocabSize] * self._numTopics)\n",
    "\n",
    "        #Assign values to parameters based on hyper-parameters\n",
    "        self.__numTAlpha = self._numTopics*self._numAlpha  \n",
    "        self.__numVBeta = self._numVocabSize*self._numBeta   \n",
    "\n",
    "        \n",
    "        for d in range(0, self._numDocSize):\n",
    "            rowOfZeros = [0] * len(self._arrDocs[d])\n",
    "            self.__arrZ.append(rowOfZeros)\n",
    "                \n",
    "    # Initialize first markov chain randomly\n",
    "    def randomMarkovChainInitialization(self):\n",
    "        \n",
    "        for d in range(self._numDocSize):\n",
    "            wta = []                        #wta - word topic assignment\n",
    "            doc = self._arrDocs[d]\n",
    "            for ind in range(len(doc)): \n",
    "                randtopic = random.randint(0, self._numTopics - 1)      # generate a topic number at random\n",
    "                self.__arrZ[d][ind] = randtopic\n",
    "                self.__arrNDT[d][randtopic] += 1\n",
    "                self.__arrNDSum[d] += 1\n",
    "                wordid = self._arrDocs[d][ind]\n",
    "                self.__arrNTW[randtopic][wordid] += 1\n",
    "                self.__arrNTSum[randtopic] += 1\n",
    "            \n",
    "    \n",
    "    #Inference (Collapsed Gibbs Sampling)\n",
    "    def gibbsSampling(self):\n",
    "        tAlpha = self._numAlpha * self._numTopics\n",
    "        vBeta = self._numBeta * self._numVocabSize            \n",
    "                    \n",
    "        for it in range(self._numGSIterations):\n",
    "            for d in range(self._numDocSize):\n",
    "                dsize = len(self._arrDocs[d])\n",
    "                for ind in range(dsize):\n",
    "                    # remove old topic from a word instance\n",
    "                    oldTopic = self.__arrZ[d][ind]\n",
    "                    wordid = self._arrDocs[d][ind]\n",
    "                    self.__arrNDT[d][oldTopic] -= 1\n",
    "                    self.__arrNDSum[d] -= 1\n",
    "                    self.__arrNTW[oldTopic][wordid] -= 1\n",
    "                    self.__arrNTSum[oldTopic] -= 1   \n",
    "\n",
    "                    # find a new more appropriate tpoic for the word instanc as per current state of the model\n",
    "                    prob = [0] * self._numTopics\n",
    "                    \n",
    "                    for t in range(self._numTopics):\n",
    "                        prob[t] = ((self.__arrNDT[d][t] + self._numAlpha) / (self.__arrNDSum[d] + tAlpha)) * \\\n",
    "                            (self.__arrNTW[t][wordid] + self._numBeta) / (self.__arrNTSum[t] + vBeta)\n",
    "                    \n",
    "                    #cumulate multinomial\n",
    "                    cdf = prob\n",
    "                    for x in range(1, len(cdf)):\n",
    "                        cdf[x] += cdf[x-1]\n",
    "                    \n",
    "                    cutoff = random.random() * cdf[-1]\n",
    "                    newTopic = 0\n",
    "                    for i in range(len(cdf)):\n",
    "                        if cdf[i] > cutoff:\n",
    "                            newTopic = i\n",
    "                            break\n",
    "                    #update as per new topic\n",
    "                    self.__arrZ[d][ind] = newTopic\n",
    "                    self.__arrNDT[d][newTopic] += 1\n",
    "                    self.__arrNDSum[d] += 1\n",
    "                    self.__arrNTW[newTopic][wordid] += 1\n",
    "                    self.__arrNTSum[newTopic] += 1\n",
    "                \n",
    "    def getTopicsPerDocument(self):\n",
    "        results = ''\n",
    "        results += \"***Topics per Document***\\n\"\n",
    "        for d in range(self._numDocSize):\n",
    "            results += \"Document \" + str(d) + \":\\n\"\n",
    "            for t in range(self._numTopics):\n",
    "                val = (self.__arrNDT[d][t]+self._numAlpha)/(self.__arrNDSum[d]+self.__numTAlpha)\n",
    "                results += \"Topic \" + str(t) + \":\" + str(val) + '\\t'\n",
    "            results += '\\n'\n",
    "        #print(results)\n",
    "        file = open('data/output-data/document-topic-distribution.txt', 'w')\n",
    "        file.write(results)\n",
    "        return results\n",
    "                    \n",
    "   \n",
    "    def getWordsPerTopic(self, revdictionary):\n",
    "        results = {}\n",
    "        \n",
    "        for t in range(self._numTopics):\n",
    "            #results += \"\\nTopic \" + str(t) + \":\"\n",
    "            #flag = 0\n",
    "            wpt = {}\n",
    "            for v in range(self._numVocabSize):\n",
    "                val = (self.__arrNTW[t][v]+self._numBeta)/(self.__arrNTSum[t]+self.__numVBeta)\n",
    "                wpt[revdictionary[str(v)]] = float(val)\n",
    "             #   flag += 1\n",
    "             #   if flag == self.__wordsPerTopic:\n",
    "             #       break\n",
    "            wpt = sorted(wpt.items(), key=lambda x: x[1], reverse=True)[:self.__wordsPerTopic]\n",
    "            results[t] = wpt\n",
    "        #print(results)\n",
    "        return results\n",
    "        \n",
    "    \n",
    "    def printall(self):\n",
    "        print(\"topics: \", self._numTopics)\n",
    "        print(\"dataset: \", self._arrDocs)\n",
    "        print(\"dataset size: \", self._numDocSize)\n",
    "        print(\"vocab: \", self._arrVocab)\n",
    "        print(\"vocab size: \", self._numVocabSize)\n",
    "        print(\"ndt: \", self.__arrNDT)\n",
    "        print(\"ndsum: \", self.__arrNDSum)\n",
    "        print(\"ntw: \", self.__arrNTW)\n",
    "        print(\"ntsum: \", self.__arrNTSum)\n",
    "        print(\"z: \", self.__arrZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4d08d3-eeb9-4641-b2bb-933d6db9c23a",
   "metadata": {},
   "source": [
    "**Running the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6d507f2-515c-4f60-9028-07e9e700064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    lda = LDA()\n",
    "    lda.getData(configurations[\"integer-encoded-doc-path\"])\n",
    "    lda.randomMarkovChainInitialization()\n",
    "    lda.gibbsSampling()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0830b3-d231-44ca-b39e-bb56ef75cf1e",
   "metadata": {},
   "source": [
    "**Results: Getting Topics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a939393a-44ad-40c2-beff-cbe8ea3e0319",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(configurations[\"integer-word-dict\"], 'r') as file:\n",
    "    revdictionary = json.load(file)\n",
    "topic_words = lda.getWordsPerTopic(revdictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d0fb59-f368-4432-88ea-c3faeb5f2281",
   "metadata": {},
   "source": [
    "### 4. Performing sentiment analysis (using SentiStrength)s\n",
    "\n",
    "- We are using Senti-Strength in this tutorial for computing the sentiment score, using scale parameter which gives a score in range [-5, 5]\n",
    "\n",
    "### 5. Separate neutral i.e., topic (aspect) presenting words and subjective words\n",
    "\n",
    "- Words with Senti-score of 0 are considered Neutral or Objective, words with score below 0 are negatively subjective while words with score above 0 are positively subjective.\n",
    "- It gives us a split of Neutral i.e., topic or aspect presentable words and subjective (both positive and negative together) words\n",
    "\n",
    "### 6. Aggregating scores of the subjective words against each topic\n",
    "- Aggregate the senti-scorse of all subjective terms in a topic (using mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "370b2726-c503-4349-bc8f-1b89f24b6722",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentistrength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dd5c6f99-6a81-4ef2-a52b-c39c096c8844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentistrength import PySentiStr\n",
    "senti = PySentiStr()\n",
    "senti.setSentiStrengthPath('jar_datei/SentiStrength.jar') # Note: Provide absolute path instead of relative path\n",
    "senti.setSentiStrengthLanguageFolderPath('SentiStrengthData/') # Note: Provide absolute path instead of relative path\n",
    "\n",
    "topic_presenting_words = {}\n",
    "topic_senti_words = {}\n",
    "topic_senti_score = {}\n",
    "\n",
    "for i in range(configurations['numTopics']):\n",
    "    topic_presenting_words[i] = []\n",
    "    topic_senti_words[i] = []\n",
    "    topic_senti_score[i] = 0\n",
    "\n",
    "for topic, wordslist in topic_words.items():\n",
    "    for word in wordslist:\n",
    "        score = senti.getSentiment(word[0], score='scale')[0]\n",
    "        if score == 0:\n",
    "            topic_presenting_words[topic].append(word[0])\n",
    "        else: \n",
    "            topic_senti_words[topic].append(word[0])\n",
    "            topic_senti_score[topic] += score\n",
    "    topic_senti_score[topic] /= len(topic_senti_words)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e488446-223c-46e4-8b66-1560db295246",
   "metadata": {},
   "source": [
    "### 7. Preparing output\n",
    "\n",
    "- Prepare understandable topic name by concating its top 5 most presentable words\n",
    "-  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d3a359ca-c7d3-4fd7-9764-364f90b6a8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic  0\n",
      "['one', 'Areas', 'experience', 'time', 'Seixo']  =  0.9 \n",
      "top remarks( great fantastic excellent kind better )\n",
      "\n",
      "Topic  1\n",
      "['stay', 'staff', 'room,', 'days', 'stay.']  =  1.5 \n",
      "top remarks( Thank wonderful beautiful thanks lovely )\n",
      "\n",
      "Topic  2\n",
      "['every', 'much', 'everything', 'Everything', 'want']  =  1.1 \n",
      "top remarks( thank enjoyed perfect love truly )\n",
      "\n",
      "Topic  3\n",
      "['come', 'back', 'hotel,', 'back.', 'next']  =  2.4 \n",
      "top remarks( amazing loved Thanks place! Fantastic )\n",
      "\n",
      "Topic  4\n",
      "['would', 'team', 'little', 'menu', 'bit']  =  0.9 \n",
      "top remarks( like good experience! beauty perfect! )\n",
      "\n",
      "Topic  5\n",
      "['The', 'hotel', 'also', 'restaurant', 'food']  =  0.6 \n",
      "top remarks( delicious friendly, Amazing impressed )\n",
      "\n",
      "Topic  6\n",
      "['made', 'And', 'always', 'get', 'guests']  =  0.3 \n",
      "top remarks( enjoy welcome )\n",
      "\n",
      "Topic  7\n",
      "['place', 'place.', 'feel', 'This', 'place,']  =  0.9 \n",
      "top remarks( special magical cozy relax everything! )\n",
      "\n",
      "Topic  8\n",
      "['The', 'room', 'staff', 'definitely', 'rooms']  =  1.3 \n",
      "top remarks( nice amazing. romantic nice. excellent. )\n",
      "\n",
      "Topic  9\n",
      "['hotel', 'could', 'One', 'since', 'night']  =  0.3 \n",
      "top remarks( best friendly specially )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(configurations['numTopics']):\n",
    "    print('Topic ', i)\n",
    "    print(topic_presenting_words[i][:5], ' = ', topic_senti_score[i], '\\ntop remarks(', ' '.join(topic_senti_words[i][:5]), ')\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13110d8-718f-4683-9cdc-808414a7a1df",
   "metadata": {},
   "source": [
    "**Commentary on Output**\n",
    "\n",
    "*Topic 0* is synonymous to user experience of the hotel getting a score of *0.9* with great, fantastic, excellent, kind and better being the top responses. *Topic 1* is similar to services getting a score of *1.5*. Similarly, *Topic 4* represents menu items getting *0.9* score etc. Some of the topics e.g., *Topic 2* is incohrent. \n",
    "\n",
    "Reducing the number of topics from 10 can results in more understandable topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f581727-06f7-4348-b801-7ceeb97718bd",
   "metadata": {},
   "source": [
    "### Bonus\n",
    "- Computing Sentiment analysis for topic words using **SentiWordNET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e1762e51-3be7-4139-98f1-f602ca720019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\khantr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\khantr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "70203b11-ade2-4035-8e83-3b0a8b11bdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic:  0 has Sentiment score of  3.375\n",
      "Topic:  1 has Sentiment score of  3.625\n",
      "Topic:  2 has Sentiment score of  2.25\n",
      "Topic:  3 has Sentiment score of  3.0\n",
      "Topic:  4 has Sentiment score of  0.625\n",
      "Topic:  5 has Sentiment score of  -0.25\n",
      "Topic:  6 has Sentiment score of  1.75\n",
      "Topic:  7 has Sentiment score of  1.75\n",
      "Topic:  8 has Sentiment score of  1.0\n",
      "Topic:  9 has Sentiment score of  1.75\n"
     ]
    }
   ],
   "source": [
    "#Sentiment Analysis with SentiWordNET (on top of WordNET)\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "for topic, wordslist in topic_words.items():\n",
    "    score = 0\n",
    "    for word in wordslist:\n",
    "        if len(wn.synsets(word[0])) > 0:\n",
    "            senti_synset = swn.senti_synset(wn.synsets(word[0])[0].name())\n",
    "            score += senti_synset.pos_score() - senti_synset.neg_score()\n",
    "    print('Topic: ', topic, 'has Sentiment score of ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84c8850-ecb9-47d0-8e72-0f77e74e3502",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
